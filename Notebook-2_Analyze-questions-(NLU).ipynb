{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze questions using NLU with a custom model\n",
    "\n",
    "This notebook demonstrates Python code for using IBM Watson Natural Language Understanding (NLU) to extract entities from text using a custom language model):\n",
    "\n",
    "- Step 0: Add project token to notebook\n",
    "- Step 1: Load questions from a file saved as an asset in the project\n",
    "- Step 2: Analyze questions using NLU\n",
    "- Step 3: Count entities\n",
    "- Step 4: New twist: *Normalize results*\n",
    "- Step 5: Save results in Watson Studio project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Add project token\n",
    "\n",
    "To be able to easily save NLU results in .csv files as assets in our Watson Studio project, we need a _project token_.\n",
    "\n",
    "Follow the steps in this topic: [Adding a project token](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/token.html?audience=wdp&context=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load questions\n",
    "\n",
    "Use the Data panel to insert the questions from the project asset file \"so_questions_watson-studio_2019-August.csv\" into the notebook code as a pandas DataFrame.\n",
    "\n",
    "See also: [Load data](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/load-and-access-data.html?audience=wdp&context=data#conns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list( df_data_1[\"Questions\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing PCA on CIFAR 10 image on IBM WATSON Studio Free version so I uploaded...\n",
      "\n",
      "I m taking the IBM Certification on Coursera and the instructions are to create ...\n",
      "\n",
      "I am following a tutorial that requires me to select the visual recognition modu...\n"
     ]
    }
   ],
   "source": [
    "print( questions[0][:80] + \"...\" + \"\\n\\n\" + questions[1][:80] + \"...\" + \"\\n\\n\" + questions[2][:80] + \"...\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analyze questions using NLU with a custom model\n",
    "\n",
    "### Natural Language Understanding API key and URL\n",
    "1. From the **Services** menu in Watson Studio, right-click \"Watson Services\" and then open the link in a new browser tab\n",
    "2. In the new Watson services tab, from the **Action** menu beside the Natural Language Understanding instance, select \"Manage in IBM Cloud\"\n",
    "3. In the service details page that opens, click **Service credentials**, then expand credentials to view them, and then copy the apikey and URL\n",
    "\n",
    "### Custom language model ID\n",
    "1. On the <b>Versions</b> page in your Knowledge Studio workspace, expand the <b>Deployed Models</b> list\n",
    "2. Copy the <b>Model ID</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = \"\" # <-- PASTE YOUR APIKEY HERE\n",
    "url    = \"\" # <-- PASTE YOUR SERVICE URL HERE\n",
    "custom_model_id = \"\" # <-- PASTE THE MODEL ID FROM KNOWLEDGE STUDIO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"ibm-watson>=3.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, ConceptsOptions, EmotionOptions, EntitiesOptions, KeywordsOptions, SemanticRolesOptions, SentimentOptions, CategoriesOptions, SyntaxOptions, SyntaxOptionsTokens\n",
    "nlu = NaturalLanguageUnderstandingV1( version='2019-07-12', iam_apikey=apikey, url=url )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze part (the first 250 characters) of the first question just to remember what the results are like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I am doing PCA on CIFAR 10 image on IBM WATSON Studio Free version so I uploaded the python file for downloading the CIFAR10 on the studio pic below. But when I trying to import cache the following error is showing. pic below- After spending some tim...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'usage': {'text_units': 1, 'text_characters': 250, 'features': 1},\n",
       " 'language': 'en',\n",
       " 'entities': [{'type': 'action',\n",
       "   'text': 'import',\n",
       "   'disambiguation': {'subtype': ['NONE']},\n",
       "   'count': 1,\n",
       "   'confidence': 0.999524},\n",
       "  {'type': 'tech',\n",
       "   'text': 'python',\n",
       "   'disambiguation': {'subtype': ['NONE']},\n",
       "   'count': 1,\n",
       "   'confidence': 0.986331},\n",
       "  {'type': 'action',\n",
       "   'text': 'downloading',\n",
       "   'disambiguation': {'subtype': ['NONE']},\n",
       "   'count': 1,\n",
       "   'confidence': 0.939268},\n",
       "  {'type': 'tech',\n",
       "   'text': 'Studio',\n",
       "   'disambiguation': {'subtype': ['NONE']},\n",
       "   'count': 1,\n",
       "   'confidence': 0.716415}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"\\n\" + questions[0][:250] + \"...\\n\" )\n",
    "nlu.analyze( text=questions[0][:250], features=Features( entities=EntitiesOptions( model=custom_model_id ) ) ).get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_questions( questions ):\n",
    "    results = []    \n",
    "    for question_txt in questions:        \n",
    "        result = nlu.analyze( text=question_txt, features=Features( entities=EntitiesOptions( model=custom_model_id ) ) ).get_result()\n",
    "        result_entities = { \"action\" : [], \"docs\" : [], \"obj\" : [], \"persona\" : [], \"tech\" : [] }\n",
    "        if( \"entities\" in result ):\n",
    "            for entity in result[\"entities\"]:\n",
    "                result_entities[ entity[\"type\"] ].append( entity[\"text\"] )\n",
    "            results.append( { \"text\"    : question_txt,\n",
    "                              \"action\"  : result_entities[\"action\"],\n",
    "                              \"docs\"    : result_entities[\"docs\"],\n",
    "                              \"obj\"     : result_entities[\"obj\"],\n",
    "                              \"persona\" : result_entities[\"persona\"],\n",
    "                              \"tech\"    : result_entities[\"tech\"] } )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlu_results = analyze_questions( questions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "   {\n",
      "      \"text\": \"I am doing PCA on CIFAR 10 image on IBM WATSON Studio Free version so I uploaded the python file for downloading the CIFAR10 on the studio pic below. But when I trying to import cache the following error is showing. pic below- After spending some time on google I find a solution but I can t understand it. link the solution is as follows - Click the Add Data icon Shows the Add Data icon and then browse the script file or drag it into your notebook sidebar. Click in an empty code cell in your notebook and then click the Insert to code link below the file. Take the returned string and write to a file in the file system that comes with the runtime session. To import the classes to access the methods in a script in your notebook use the following command For Python from python file name import class name I can t understand this line and write to a file in the file system that comes with the runtime session. Where can I find the file that comes with runtime session? Where is the file system\",\n",
      "      \"action\": [\n",
      "         \"import\",\n",
      "         \"import\",\n",
      "         \"import\",\n",
      "         \"downloading\",\n",
      "         \"Add\",\n",
      "         \"Add\",\n",
      "         \"access\"\n",
      "      ],\n",
      "      \"docs\": [],\n",
      "      \"obj\": [\n",
      "         \"notebook\",\n",
      "         \"notebook\",\n",
      "         \"notebook\",\n",
      "         \"methods\"\n",
      "      ],\n",
      "      \"persona\": [],\n",
      "      \"tech\": [\n",
      "         \"python\",\n",
      "         \"python\",\n",
      "         \"Python\",\n",
      "         \"Studio\"\n",
      "      ]\n",
      "   },\n",
      "   {\n",
      "      \"text\": \"I m taking the IBM Certification on Coursera and the instructions are to create a project and then select visual recognition project but after an apparent redesign the 2nd option is no longer there. I read one previous post about this issue but it did not solve my problem as the create button is grayed out and I m not sure what to do to create this project. Any tips would be appreciated\",\n",
      "      \"action\": [\n",
      "         \"create\",\n",
      "         \"create\",\n",
      "         \"create\"\n",
      "      ],\n",
      "      \"docs\": [],\n",
      "      \"obj\": [\n",
      "         \"project\",\n",
      "         \"project\",\n",
      "         \"project\"\n",
      "      ],\n",
      "      \"persona\": [],\n",
      "      \"tech\": []\n",
      "   },\n",
      "   {\n",
      "      \"text\": \"I am following a tutorial that requires me to select the visual recognition module as the next step of selecting create a project. However after clicking create project I only see two options Create an empty project Create a project from a sample or file Could you please let me know how to access the visual recognition module?\",\n",
      "      \"action\": [\n",
      "         \"access\",\n",
      "         \"create\",\n",
      "         \"select\",\n",
      "         \"Create\",\n",
      "         \"Create\",\n",
      "         \"create\",\n",
      "         \"selecting\"\n",
      "      ],\n",
      "      \"docs\": [\n",
      "         \"tutorial\"\n",
      "      ],\n",
      "      \"obj\": [\n",
      "         \"project\",\n",
      "         \"project\",\n",
      "         \"project\",\n",
      "         \"project\"\n",
      "      ],\n",
      "      \"persona\": [],\n",
      "      \"tech\": []\n",
      "   },\n",
      "   {\n",
      "      \"text\": \"I have a cloud object storage in IBM cloud and a bucket inside it. there are .wav files inside the bucket and I have established a connection to my project in watson studio I have added the data asset too. but I am not able to insert the data into python jupyter notebook. I only get the insert the credential in the find add data option not the data as a streaming object. I used to do it before some days. I am using a lite version and my account free trial for a mont is valid till 12th Aug 2019\",\n",
      "      \"action\": [\n",
      "         \"add\",\n",
      "         \"connection\",\n",
      "         \"credential\"\n",
      "      ],\n",
      "      \"docs\": [],\n",
      "      \"obj\": [\n",
      "         \"notebook\",\n",
      "         \"project\",\n",
      "         \"account\",\n",
      "         \"free trial\"\n",
      "      ],\n",
      "      \"persona\": [],\n",
      "      \"tech\": [\n",
      "         \"python\",\n",
      "         \"object storage\",\n",
      "         \"jupyter\",\n",
      "         \"studio\",\n",
      "         \"IBM cloud\"\n",
      "      ]\n",
      "   },\n",
      "   {\n",
      "      \"text\": \"I am using project lib in ibm watson studio to store and save files created in a jupyter notebook mainly csv and pkl files . The thing is that I need to store these files in different folders in my project. I could not find the way to do this properly. def my_append_data source_data file_name source data is a Pandas Dataframe project.save_data file_name source_data.to_csv set_project_asset True overwrite True save csv file return 0 this is the function that i am using for csv files but the files are stored in the project library without any folder specification is there any way to create folders inside the project using project lib?\",\n",
      "      \"action\": [\n",
      "         \"create\"\n",
      "      ],\n",
      "      \"docs\": [],\n",
      "      \"obj\": [\n",
      "         \"project\",\n",
      "         \"project\",\n",
      "         \"notebook\",\n",
      "         \"project\",\n",
      "         \"project\",\n",
      "         \"project\",\n",
      "         \"Dataframe\"\n",
      "      ],\n",
      "      \"persona\": [],\n",
      "      \"tech\": [\n",
      "         \"jupyter\",\n",
      "         \"Pandas\"\n",
      "      ]\n",
      "   }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print( json.dumps( nlu_results, indent=3 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Count Entities\n",
    "\n",
    "To perform analysis and create nice charts, we need to tally results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def count_tags( results_list ):\n",
    "    tags = {}\n",
    "    for tag_arr in results_list:\n",
    "        for tag in tag_arr:\n",
    "            tag = tag.lower()\n",
    "            if( \"\" != tag ):\n",
    "                if( tag not in tags ):\n",
    "                    tags[tag] = 0\n",
    "                tags[tag] += 1\n",
    "    common_tags = dict( [ (k,v) for k,v in tags.items() ] )\n",
    "    ordered_common_tags = OrderedDict( sorted( common_tags.items(), key=lambda x:x[1], reverse=True ) )\n",
    "    return ordered_common_tags\n",
    "    \n",
    "def count_words( results_list, entity_type ):\n",
    "    entities = {}\n",
    "    for entry in results_list:\n",
    "        words = entry[entity_type]\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if( \"\" != word ):\n",
    "                if( word not in entities ):\n",
    "                    entities[word] = 0\n",
    "                entities[word] += 1\n",
    "    common_entities = dict( [ (k,v) for k,v in entities.items() ] )\n",
    "    ordered_common_entities = OrderedDict( sorted( common_entities.items(), key=lambda x:x[1], reverse=True ) )\n",
    "    return ordered_common_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_counts  = count_words( nlu_results, \"action\" )\n",
    "obj_counts     = count_words( nlu_results, \"obj\" )\n",
    "tech_counts    = count_words( nlu_results, \"tech\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('create', 8),\n",
       "             ('import', 3),\n",
       "             ('add', 3),\n",
       "             ('access', 2),\n",
       "             ('downloading', 1),\n",
       "             ('select', 1),\n",
       "             ('selecting', 1),\n",
       "             ('connection', 1),\n",
       "             ('credential', 1)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('project', 13),\n",
       "             ('notebook', 5),\n",
       "             ('methods', 1),\n",
       "             ('account', 1),\n",
       "             ('free trial', 1),\n",
       "             ('dataframe', 1)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('python', 4),\n",
       "             ('studio', 2),\n",
       "             ('jupyter', 2),\n",
       "             ('object storage', 1),\n",
       "             ('ibm cloud', 1),\n",
       "             ('pandas', 1)])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Normalize results\n",
    "\n",
    "There are some noisy results above.  For example, in `action_counts`, \"select\" and \"selecting\" are counted as separate entities.  But for our analysis purposes, those both refer to the same action.  Instead of being counted separately, they should be counted together.\n",
    "\n",
    "#### Dictionary files\n",
    "\n",
    "To train the custom language model, we created dictionary files that looked like this:\n",
    "\n",
    "[Action words](https://raw.githubusercontent.com/spackows/CASCON-2017_Analyzing_chat/master/custom-language-model/dictionaries/action.csv)\n",
    "\n",
    "```\n",
    "lemma,poscode,surface\n",
    "select,2,select,selecting\n",
    "create,2,create,creating\n",
    "train,2,train,training\n",
    "load,2,load,loading,upload,uploading\n",
    "sign up,2,sign up,sign-up,signup,register,registering\n",
    "import,2,import,importing,imported\n",
    "...\n",
    "```\n",
    "\n",
    "Given that we went to the trouble of creating those dictionaries, let's use them to *normalize* results.  For example, count \"select\" and \"selecting\" as two instances of the same action.\n",
    "\n",
    "#### Method: _lookup_\n",
    "\n",
    "The way we'll use those dictionary files to normalize results is this:\n",
    "\n",
    "From the dictionaries, create an important words look-up structure that we can use to map any `surface` form back to the `lemma` form.\n",
    "\n",
    "For example, using the action words dictionary, \"loading\", \"upload\", and \"uploading\" should all map back to: \"load\".\n",
    "\n",
    "#### Other methods\n",
    "\n",
    "We could use *stemming* or *lemmatization* libraries.. But why?  We already have this dictionaries of words we care about, so let's just use those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def readSource( url ):\n",
    "    content = urllib.request.urlopen( url )\n",
    "    lines_arr = []\n",
    "    for line in content:\n",
    "        lines_arr.append( line.decode(\"utf-8\") )\n",
    "    return lines_arr\n",
    "\n",
    "def addLookups( lines_arr, lookup_dict ):\n",
    "    for i in range( 1, len( lines_arr ) ):\n",
    "        line = lines_arr[i]\n",
    "        line = re.sub( \"\\s+$\", \"\", line )\n",
    "        arr = line.split( \",\" )\n",
    "        lemma = arr[0].lower()\n",
    "        for j in range( 3, len( arr ) ):\n",
    "            variant = arr[j].lower()\n",
    "            if variant not in lookup_dict:\n",
    "                lookup_dict[ variant ] = lemma\n",
    "    return lookup_dict\n",
    "\n",
    "def readCustomDictionaries( url_arr ):\n",
    "    lookup_dict = {}\n",
    "    for url in url_arr:\n",
    "        lines_arr = readSource( url )\n",
    "        lookup_dict = addLookups( lines_arr, lookup_dict )\n",
    "    return lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict_url = \"https://raw.githubusercontent.com/spackows/CASCON-2017_Analyzing_chat/master/custom-language-model/dictionaries/action.csv\"\n",
    "obj_dict_url    = \"https://raw.githubusercontent.com/spackows/CASCON-2017_Analyzing_chat/master/custom-language-model/dictionaries/obj.csv\"\n",
    "tech_dict_url   = \"https://raw.githubusercontent.com/spackows/CASCON-2017_Analyzing_chat/master/custom-language-model/dictionaries/tech.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selecting': 'select',\n",
       " 'creating': 'create',\n",
       " 'training': 'train',\n",
       " 'loading': 'load',\n",
       " 'upload': 'load',\n",
       " 'uploading': 'load',\n",
       " 'sign-up': 'sign up',\n",
       " 'signup': 'sign up',\n",
       " 'register': 'sign up',\n",
       " 'registering': 'sign up',\n",
       " 'importing': 'import',\n",
       " 'imported': 'import',\n",
       " 'adding': 'add',\n",
       " 'recovering': 'recover',\n",
       " 'changing': 'change',\n",
       " 'building': 'build',\n",
       " 'login': 'log in',\n",
       " 'logging in': 'log in',\n",
       " 'sign in': 'log in',\n",
       " 'signing in': 'log in',\n",
       " 'sign-in': 'log in',\n",
       " 'signin': 'log in',\n",
       " 'connecting': 'connect',\n",
       " 'connection': 'connect',\n",
       " 'connections': 'connect',\n",
       " 'deploying': 'deploy',\n",
       " 'setting up': 'set up',\n",
       " 'setup': 'set up',\n",
       " 'set-up': 'set up',\n",
       " 'editing': 'edit',\n",
       " 'exceeds': 'exceed',\n",
       " 'exceeded': 'exceed',\n",
       " 'exceeding': 'exceed',\n",
       " 'exporting': 'export',\n",
       " 'analyzing': 'analyze',\n",
       " 'downloading': 'download',\n",
       " 'accessing': 'access',\n",
       " 'acess': 'access',\n",
       " 'saving': 'save',\n",
       " 'initiating': 'initiate',\n",
       " 'preparing': 'prepare',\n",
       " 'requesting': 'request',\n",
       " 'writing': 'write',\n",
       " 'renaming': 'rename',\n",
       " 'orgs': 'org',\n",
       " 'organization': 'org',\n",
       " 'organizations': 'org',\n",
       " 'models': 'model',\n",
       " 'accounts': 'account',\n",
       " 'acount': 'account',\n",
       " 'acont': 'account',\n",
       " 'accont': 'account',\n",
       " 'accout': 'account',\n",
       " 'projects': 'project',\n",
       " 'names': 'name',\n",
       " 'datasets': 'dataset',\n",
       " 'data set': 'dataset',\n",
       " 'data sets': 'dataset',\n",
       " 'shapefiles': 'shapefile',\n",
       " 'shape file': 'shapefile',\n",
       " 'shape files': 'shapefile',\n",
       " 'access keys': 'access key',\n",
       " 'endpoints': 'endpoint',\n",
       " 'end point': 'endpoint',\n",
       " 'end points': 'endpoint',\n",
       " 'scoring endpoint': 'endpoint',\n",
       " 'scoring endpoints': 'endpoint',\n",
       " 'scoring end point': 'endpoint',\n",
       " 'scoring end points': 'endpoint',\n",
       " 'limits': 'limit',\n",
       " 'free trial': 'trial',\n",
       " 'trail': 'trial',\n",
       " 'free plan': 'trial',\n",
       " 'free access': 'trial',\n",
       " 'collaborators': 'collaborator',\n",
       " 'flows': 'flow',\n",
       " 'data frame': 'dataframe',\n",
       " 'databases': 'database',\n",
       " 'db': 'database',\n",
       " 'dbm': 'database',\n",
       " 'dbs': 'database',\n",
       " 'note book': 'notebook',\n",
       " 'note-book': 'notebook',\n",
       " 'notebooks': 'notebook',\n",
       " 'note-books': 'notebook',\n",
       " 'note books': 'notebook',\n",
       " 'watson machine learning': 'machine learning',\n",
       " 'ibm watson machine learning': 'machine learning',\n",
       " 'ibm watson machine learning (spss)': 'machine learning',\n",
       " 'spss': 'machine learning',\n",
       " 'spss modeler': 'machine learning',\n",
       " 'modeler': 'machine learning',\n",
       " 'wml': 'machine learning',\n",
       " 'ml': 'machine learning',\n",
       " 'watsonml': 'machine learning',\n",
       " 'r studio': 'r',\n",
       " 'rstudio': 'r',\n",
       " 'r-studio': 'r',\n",
       " 'object-storage': 'object storage',\n",
       " 'objectstorage': 'object storage',\n",
       " 'apis': 'api',\n",
       " 'api': 'api',\n",
       " 'sparks': 'spark',\n",
       " 'pyspark': 'spark',\n",
       " 'git': 'github',\n",
       " 'clodant': 'cloudant',\n",
       " 'cloudent nosql': 'cloudant',\n",
       " 'cloudent nosql db': 'cloudant',\n",
       " 'nodered': 'node-red',\n",
       " 'node red': 'node-red',\n",
       " 'db2': 'db2',\n",
       " 'dash': 'db2',\n",
       " 'dashdb': 'db2'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_struct = readCustomDictionaries( [ action_dict_url, obj_dict_url, tech_dict_url ] )\n",
    "lookup_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize( word, lookup_struct ):\n",
    "    if word in lookup_struct:\n",
    "        return lookup_struct[word]\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words2( results_list, entity_type, lookup_struct ):\n",
    "    entities = {}\n",
    "    for entry in results_list:\n",
    "        words = entry[entity_type]\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = normalize( word, lookup_struct );\n",
    "            if( \"\" != word ):\n",
    "                if( word not in entities ):\n",
    "                    entities[word] = 0\n",
    "                entities[word] += 1\n",
    "    common_entities = dict( [ (k,v) for k,v in entities.items() ] )\n",
    "    ordered_common_entities = OrderedDict( sorted( common_entities.items(), key=lambda x:x[1], reverse=True ) )\n",
    "    return ordered_common_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_counts2 = count_words2( nlu_results, \"action\", lookup_struct )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('create', 8),\n",
       "             ('import', 3),\n",
       "             ('add', 3),\n",
       "             ('access', 2),\n",
       "             ('downloading', 1),\n",
       "             ('select', 1),\n",
       "             ('selecting', 1),\n",
       "             ('connection', 1),\n",
       "             ('credential', 1)])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('create', 8),\n",
       "             ('import', 3),\n",
       "             ('add', 3),\n",
       "             ('access', 2),\n",
       "             ('select', 2),\n",
       "             ('download', 1),\n",
       "             ('connect', 1),\n",
       "             ('credential', 1)])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_counts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save results in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrameCSV( counts ):\n",
    "    data = { \"Entities\": list( counts.keys() ),  \"Counts\": list( counts.values() ) }\n",
    "    return pd.DataFrame( data=data ).to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'so_entities-tech_watson-studio_2019-August.csv',\n",
       " 'message': 'File saved to project storage.',\n",
       " 'bucket_name': 'nluworkshopproj-donotdelete-pr-kxjtz2yxb1sovi',\n",
       " 'asset_id': '8e1c13b5-fc71-4225-b8f8-8d0904278d46'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.save_data( \"so_entities-action_watson-studio_2019-August.csv\", createDataFrameCSV( action_counts2 ), overwrite=True )\n",
    "project.save_data( \"so_entities-obj_watson-studio_2019-August.csv\",    createDataFrameCSV( obj_counts     ), overwrite=True )\n",
    "project.save_data( \"so_entities-tech_watson-studio_2019-August.csv\",   createDataFrameCSV( tech_counts    ), overwrite=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
